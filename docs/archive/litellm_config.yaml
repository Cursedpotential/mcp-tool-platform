# LiteLLM Configuration for Salem Forensics MCP Tool Platform
# This file defines model routing, fallbacks, and cost tracking

model_list:
  # ============================================================================
  # OpenAI Models
  # ============================================================================
  - model_name: gpt-4-turbo
    litellm_params:
      model: gpt-4-turbo-preview
      api_key: os.environ/OPENAI_API_KEY
      rpm: 10000
      tpm: 2000000
  
  - model_name: gpt-4
    litellm_params:
      model: gpt-4
      api_key: os.environ/OPENAI_API_KEY
      rpm: 10000
      tpm: 300000
  
  - model_name: gpt-3.5-turbo
    litellm_params:
      model: gpt-3.5-turbo
      api_key: os.environ/OPENAI_API_KEY
      rpm: 10000
      tpm: 2000000

  # ============================================================================
  # Anthropic Models
  # ============================================================================
  - model_name: claude-3-opus
    litellm_params:
      model: claude-3-opus-20240229
      api_key: os.environ/ANTHROPIC_API_KEY
      rpm: 4000
      tpm: 400000
  
  - model_name: claude-3-sonnet
    litellm_params:
      model: claude-3-5-sonnet-20241022
      api_key: os.environ/ANTHROPIC_API_KEY
      rpm: 4000
      tpm: 400000
  
  - model_name: claude-3-haiku
    litellm_params:
      model: claude-3-haiku-20240307
      api_key: os.environ/ANTHROPIC_API_KEY
      rpm: 4000
      tpm: 400000

  # ============================================================================
  # Google Gemini Models
  # ============================================================================
  - model_name: gemini-pro
    litellm_params:
      model: gemini/gemini-pro
      api_key: os.environ/GEMINI_API_KEY
      rpm: 60
      tpm: 32000
  
  - model_name: gemini-flash
    litellm_params:
      model: gemini/gemini-1.5-flash
      api_key: os.environ/GEMINI_API_KEY
      rpm: 1000
      tpm: 4000000

  # ============================================================================
  # Cohere Models
  # ============================================================================
  - model_name: command-r-plus
    litellm_params:
      model: command-r-plus
      api_key: os.environ/COHERE_API_KEY
      rpm: 10000
      tpm: 1000000

  # ============================================================================
  # Groq Models (Fast inference)
  # ============================================================================
  - model_name: llama-3-70b
    litellm_params:
      model: groq/llama3-70b-8192
      api_key: os.environ/GROQ_API_KEY
      rpm: 30
      tpm: 14400

  # ============================================================================
  # OpenRouter (Multi-model gateway)
  # ============================================================================
  - model_name: openrouter-auto
    litellm_params:
      model: openrouter/auto
      api_key: os.environ/OPENROUTER_API_KEY
      rpm: 200
      tpm: 200000

  # ============================================================================
  # Local Models via Ollama
  # ============================================================================
  - model_name: llama3-local
    litellm_params:
      model: ollama/llama3
      api_base: http://ollama:11434
      rpm: 1000
      tpm: 1000000

# ============================================================================
# Router Settings - Define fallback chains and load balancing
# ============================================================================
router_settings:
  routing_strategy: usage-based-routing  # Options: simple-shuffle, latency-based-routing, usage-based-routing
  
  # Fallback chains for different task types
  model_group_alias:
    # High-quality analysis (forensic reports, legal documents)
    forensic-analysis:
      - claude-3-opus
      - gpt-4-turbo
      - claude-3-sonnet
    
    # Fast classification (sentiment, pattern detection)
    fast-classification:
      - gemini-flash
      - claude-3-haiku
      - gpt-3.5-turbo
      - llama-3-70b
    
    # Code generation (agent builders, workflow scripts)
    code-generation:
      - gpt-4-turbo
      - claude-3-sonnet
      - command-r-plus
    
    # Bulk processing (embedding, summarization)
    bulk-processing:
      - gemini-flash
      - gpt-3.5-turbo
      - llama3-local
    
    # Budget-friendly (testing, development)
    budget:
      - gemini-flash
      - llama3-local
      - openrouter-auto

# ============================================================================
# Caching Settings
# ============================================================================
cache:
  type: redis
  host: redis
  port: 6379
  password: os.environ/REDIS_PASSWORD
  ttl: 3600  # Cache responses for 1 hour
  
  # Cache rules
  cache_responses: true
  cache_kwargs:
    similarity_threshold: 0.9  # Cache if 90% similar to previous request

# ============================================================================
# Cost Tracking & Budgets
# ============================================================================
litellm_settings:
  # Enable detailed logging
  success_callback: ["langfuse"]  # TODO: Set up Langfuse for observability
  failure_callback: ["langfuse"]
  
  # Budget limits (per day)
  max_budget: 100.0  # $100/day total
  budget_duration: 1d
  
  # Per-user budgets (TODO: Implement user-based tracking)
  # user_api_key_budget:
  #   user_123: 10.0  # $10/day per user

# ============================================================================
# General Settings
# ============================================================================
general_settings:
  master_key: os.environ/LITELLM_MASTER_KEY
  database_url: os.environ/DATABASE_URL
  
  # Retry settings
  num_retries: 3
  request_timeout: 600
  
  # Rate limiting
  rpm: 10000
  tpm: 2000000
  
  # Logging
  set_verbose: true
  json_logs: true

# ============================================================================
# TODO: Implement these features
# ============================================================================
# - Add user-based API key management
# - Set up Langfuse for observability
# - Configure custom model endpoints
# - Add A/B testing for model comparison
# - Implement cost alerts (email/webhook when budget exceeded)
# - Add model performance tracking (latency, error rate)
