version: '3.8'

# ============================================================================
# Optimized Docker Compose for 8GB/4-core VPS
# ============================================================================
#
# Service tiers:
# - CORE (always-on): Chroma, LiteLLM, MetaMCP, Directus, n8n, Tailscale
# - ON-DEMAND (webhook-controlled): Kasm, LibreChat, Open WebUI, Ollama, Browserless, Playwright
#
# Total RAM usage:
# - Core services: ~2.5GB
# - All on-demand running: ~5GB
# - Total: ~7.5GB (leaves 500MB for OS)
#
# External services (not in this compose):
# - Neo4j Aura (hosted)
# - PhotoPrism (separate cheap VPS)
# - Jupyter (use Google Colab)
# - Metabase (custom D3/ECharts dashboard in Manus app)
# - Redis (commented out, use Dragonfly later)
#
# ============================================================================

services:
  # ============================================================================
  # CORE SERVICES (Always-On)
  # ============================================================================

  # Chroma VPS - Persistent vector database (~512MB)
  chroma:
    image: chromadb/chroma:latest
    container_name: salem-chroma
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/chroma/chroma
    environment:
      - CHROMA_SERVER_AUTH_CREDENTIALS_PROVIDER=chromadb.auth.token.TokenConfigServerAuthCredentialsProvider
      - CHROMA_SERVER_AUTH_TOKEN_TRANSPORT_HEADER=X-Chroma-Token
      - CHROMA_SERVER_AUTH_CREDENTIALS=${CHROMA_AUTH_TOKEN}
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
    restart: unless-stopped
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M

  # LiteLLM - Universal LLM proxy (~512MB)
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: salem-litellm
    ports:
      - "4000:4000"
    environment:
      - DATABASE_URL=postgresql://${SUPABASE_USER}:${SUPABASE_PASSWORD}@${SUPABASE_HOST}:5432/${SUPABASE_DATABASE}?sslmode=require
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY}
      - LITELLM_LOG=INFO
      - LITELLM_DROP_PARAMS=true
      # Model API keys
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - COHERE_API_KEY=${COHERE_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: --config /app/config.yaml --port 4000 --num_workers 2
    restart: unless-stopped
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M

  # MetaMCP - MCP server registry (~256MB)
  metamcp:
    build:
      context: .
      dockerfile: Dockerfile.metamcp
    container_name: salem-metamcp
    ports:
      - "4001:4001"
    environment:
      - PORT=4001
      - DATABASE_URL=postgresql://${SUPABASE_USER}:${SUPABASE_PASSWORD}@${SUPABASE_HOST}:5432/${SUPABASE_DATABASE}?sslmode=require
      - MCP_REGISTRY_URL=https://mcp.run/registry
    volumes:
      - ./server/mcp:/app/mcp
      - metamcp_data:/app/data
    restart: unless-stopped
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 256M

  # Directus - Headless CMS with R2 storage (~512MB)
  directus:
    image: directus/directus:latest
    container_name: salem-directus
    ports:
      - "8055:8055"
    environment:
      - DB_CLIENT=pg
      - DB_HOST=${SUPABASE_HOST}
      - DB_PORT=5432
      - DB_DATABASE=${SUPABASE_DATABASE}
      - DB_USER=${SUPABASE_USER}
      - DB_PASSWORD=${SUPABASE_PASSWORD}
      - DB_SSL=true
      - ADMIN_EMAIL=${DIRECTUS_ADMIN_EMAIL}
      - ADMIN_PASSWORD=${DIRECTUS_ADMIN_PASSWORD}
      - KEY=${DIRECTUS_KEY}
      - SECRET=${DIRECTUS_SECRET}
      - STORAGE_LOCATIONS=r2
      - STORAGE_R2_DRIVER=s3
      - STORAGE_R2_KEY=${R2_ACCESS_KEY_ID}
      - STORAGE_R2_SECRET=${R2_SECRET_ACCESS_KEY}
      - STORAGE_R2_BUCKET=${R2_BUCKET}
      - STORAGE_R2_REGION=auto
      - STORAGE_R2_ENDPOINT=${R2_ENDPOINT}
      - STORAGE_R2_ACL=private
      - PUBLIC_URL=${DIRECTUS_PUBLIC_URL}
      - MAX_PAYLOAD_SIZE=100mb
      - CACHE_ENABLED=false
    restart: unless-stopped
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M

  # n8n - Workflow automation (~512MB)
  n8n:
    image: n8nio/n8n:latest
    container_name: salem-n8n
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_USER}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_PASSWORD}
      - N8N_HOST=localhost
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - WEBHOOK_URL=http://localhost:5678/
      - GENERIC_TIMEZONE=America/New_York
      # Docker socket access for container control
      - DOCKER_HOST=unix:///var/run/docker.sock
    volumes:
      - n8n_data:/home/node/.n8n
      - /var/run/docker.sock:/var/run/docker.sock
    restart: unless-stopped
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M

  # Tailscale - Secure VPN (~128MB)
  tailscale:
    image: tailscale/tailscale:latest
    container_name: salem-tailscale
    hostname: salem-forensics
    environment:
      - TS_AUTHKEY=${TAILSCALE_AUTH_KEY}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_EXTRA_ARGS=--advertise-tags=tag:salem
    volumes:
      - tailscale_data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - SYS_MODULE
    restart: unless-stopped
    network_mode: host
    deploy:
      resources:
        limits:
          memory: 128M

  # ============================================================================
  # ON-DEMAND SERVICES (Webhook-controlled via n8n)
  # ============================================================================

  # Kasm Workspace - Debian desktop with AI CLIs (~2GB)
  kasm-workspace:
    build:
      context: .
      dockerfile: Dockerfile.kasm
    container_name: salem-kasm-workspace
    ports:
      - "6901:6901"
    environment:
      - VNC_PW=${KASM_VNC_PASSWORD}
      - VNC_RESOLUTION=1920x1080
      - VNC_COL_DEPTH=24
      - R2_ACCESS_KEY_ID=${R2_ACCESS_KEY_ID}
      - R2_SECRET_ACCESS_KEY=${R2_SECRET_ACCESS_KEY}
      - R2_ENDPOINT=${R2_ENDPOINT}
      - CLAUDE_API_KEY=${ANTHROPIC_API_KEY}
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
    volumes:
      - kasm_home:/home/kasm-user
      - kasm_config:/home/kasm-user/.config
      - ./scripts:/scripts:ro
      - ./data:/data
    shm_size: 2gb
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 2G
    labels:
      - "n8n.service=on-demand"
      - "n8n.priority=low"

  # LibreChat - Multi-model chat UI (~1GB)
  librechat:
    image: ghcr.io/danny-avila/librechat:latest
    container_name: salem-librechat
    ports:
      - "3002:3080"
    environment:
      - HOST=0.0.0.0
      - PORT=3080
      - MONGO_URI=mongodb://mongo:27017/LibreChat
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - GOOGLE_API_KEY=${GEMINI_API_KEY}
      - OPENROUTER_API_KEY=${OPENROUTER_API_KEY}
    depends_on:
      - mongo
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 1G
    labels:
      - "n8n.service=on-demand"
      - "n8n.priority=medium"

  # MongoDB for LibreChat (~512MB)
  mongo:
    image: mongo:7
    container_name: salem-mongo
    volumes:
      - mongo_data:/data/db
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M

  # Open WebUI - ChatGPT-style UI (~512MB)
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: salem-open-webui
    ports:
      - "3003:8080"
    environment:
      - WEBUI_SECRET_KEY=${OPENWEBUI_SECRET_KEY}
      - WEBUI_NAME=Salem Forensics AI
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_API_BASE_URL=https://api.openai.com/v1
      - OLLAMA_BASE_URL=http://ollama:11434
      - ENABLE_RAG=true
      - ENABLE_SIGNUP=false
    volumes:
      - open_webui_data:/app/backend/data
    depends_on:
      - ollama
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M
    labels:
      - "n8n.service=on-demand"
      - "n8n.priority=low"

  # Ollama - Local LLM runtime (routing only, ~512MB)
  ollama:
    image: ollama/ollama:latest
    container_name: salem-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M
    labels:
      - "n8n.service=on-demand"
      - "n8n.priority=low"

  # Browserless - Headless Chrome (~512MB)
  browserless:
    image: browserless/chrome:latest
    container_name: salem-browserless
    ports:
      - "3004:3000"
    environment:
      - TOKEN=${BROWSERLESS_TOKEN}
      - MAX_CONCURRENT_SESSIONS=2
      - CONNECTION_TIMEOUT=60000
      - MAX_QUEUE_LENGTH=5
      - ENABLE_DEBUGGER=true
      - PREBOOT_CHROME=false
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 512M
    labels:
      - "n8n.service=on-demand"
      - "n8n.priority=medium"

  # Playwright - Headless browser automation (~256MB)
  playwright:
    build:
      context: .
      dockerfile: Dockerfile.playwright
    container_name: salem-playwright
    ports:
      - "3005:3000"
    environment:
      - PLAYWRIGHT_BROWSERS_PATH=/ms-playwright
    volumes:
      - playwright_cache:/ms-playwright
      - ./scripts/playwright:/scripts
    restart: "no"  # Start via webhook
    networks:
      - salem-network
    deploy:
      resources:
        limits:
          memory: 256M
    labels:
      - "n8n.service=on-demand"
      - "n8n.priority=medium"

  # ============================================================================
  # COMMENTED OUT SERVICES (Add later when needed)
  # ============================================================================

  # # Redis - Caching (use Dragonfly instead)
  # redis:
  #   image: redis:7-alpine
  #   container_name: salem-redis
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
  #   restart: unless-stopped
  #   networks:
  #     - salem-network

  # # Apache Superset - BI platform (add later, ~1GB)
  # superset:
  #   image: apache/superset:latest
  #   container_name: salem-superset
  #   ports:
  #     - "8088:8088"
  #   environment:
  #     - SUPERSET_SECRET_KEY=${SUPERSET_SECRET_KEY}
  #   volumes:
  #     - superset_data:/app/superset_home
  #   restart: unless-stopped
  #   networks:
  #     - salem-network

volumes:
  chroma_data:
  metamcp_data:
  n8n_data:
  tailscale_data:
  kasm_home:
  kasm_config:
  mongo_data:
  open_webui_data:
  ollama_data:
  playwright_cache:

networks:
  salem-network:
    driver: bridge
